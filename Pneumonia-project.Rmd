---
title: "Pneumonia identification - ML image recognition"
author: "Iuliia Shal"
date: "10 02 2022"
geometry: margin=1in
fontsize: 12pt
spacing: double
theme: united
fig.align: center
header-includes:
  - |
    ```{=latex}
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      breaksymbolleft={},
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
    ```
    
output: 
  pdf_document:
    df_print: kable
    dev: png
    toc: true
    number_sections: true
    highlight: tango
    fig_width: 8
    fig_height: 5

---
```{r, global_options, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  tidy = TRUE,
  tidy.opts = list(arrow = TRUE, indent = 2, width.cutoff = 60),
  fig.align = "center"
)

Sys.setlocale("LC_ALL", "English")
```
\
```{r setup, include=FALSE}
library(tidyverse)
library(imager)
library(opencv)
library(caret)
library(dplyr)
library(matrixStats)
library(randomForest)
library(rpart)
```

#   Project goal
For decades, Pneumonia has remained the leading cause of death due to infectious disease around the world. According to the World Health Organization, Pneumonia killed more than $808,000$ children under the age of 5 in 2017, accounting for $15\%$ of all deaths of children under 5 years.\

The aim of this project is to identify pneumonia diagnosis for children between 1 and 5 years old in the early stages with the highest accuracy.\

The initial source we will use in this project: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia \
\
The dataset is organized into 3 folders (train, test, val) and contains sub-folders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal). \

Chest X-Ray images were selected from retrospective cohorts of pediatric patients of 1 to 5 years old from Guangzhou Women and Children Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care.

For the analysis of chest x-ray images, all chest radiography were initially screened for quality control by removing all low-quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.

#   Import the Chest X-ray dataset

To download dataset directly from Kaggle is quite challenging  if you don't use Python on your computer, so as a result I decided to load it on the Shared Drive for the external users.

Please make sure to install "googledrive" package:\

```{r setup googledrive, include=TRUE, warning=FALSE}
library("googledrive")
```
As mentioned previously, there are 3 datasets: $train$, $test$ and $val$ and each of them has its URL with zip to download and folder to which it will be unzipped.
```{r setup googledrive url, include=TRUE}
train_url <- "https://drive.google.com/file/d/1UEdxXC2JuQ9ZK9ZcuH1Txa14xn0aFpc9/download"
test_url <- "https://drive.google.com/file/d/1UF6JKw7O4qSwxOrIp-k3YXp_a-15EDGr/download"
val_url <- "https://drive.google.com/file/d/1UEDMUIwL_8RIOK4fMPP7lTFv8p2VM_sH/download"

val_zip <- "val.zip"
test_zip <- "test.zip"
train_zip <- "train.zip"

val_folder <- "val"
test_folder <- "test"
train_folder <- "train"
```

In the next section we will explore how we can quickly load 2GB of data from Google Drive to R.


#   Data exploration
##  Load data

Let's have a look at the examples of chest X-rays we have in datasets:
```{r photo examples, echo=FALSE}
drive_download(val_url)
out <- unzip(val_zip)
list_subfolders <- list.dirs(path = val_folder, full.names = TRUE, recursive = FALSE)
filenames <- lapply(list_subfolders, function(x) list.files(path = x, pattern = "*.jpeg", full.names = TRUE))
filenames <- do.call(c, filenames)
file.remove(val_zip) # delete zip

Normal <- load.image(filenames[3])
Pneumonia <- load.image(filenames[15])
unlink(val_folder, recursive = TRUE) # delete temp photos

layout(t(1:2))
plot(Normal, main = "Normal X-Ray example")
plot(Pneumonia, main = "Pneumonia X-Ray example")
```
We can see that the normal chest X-ray (left panel) depicts clear lungs without any areas of abnormal opacification in the image, meanwhile viral pneumonia (right) manifests with a  ‘foggier’ pattern in both lungs.
\
```{r Dimensions exploration, echo=FALSE}
Normal
Pneumonia
```
We notice that photos have different dimensions and, moreover, during investigation it was found out that some photos have 3 color channels (RGB), not just 1 Black & White channel as per our example. As a result, we can't collect such inconsistent data into one dataframe. 
To fix this issue we need to covert all photos to gray scale and resize them to one format. It was decided to convert them to pictures of 100x100 size ($10,000$ pixels) so that machine learning models could work on the average computer and run within a couple of hours.\

*Approximate time of running the whole code $\approx$ 5 hours.*\
```{r delete, echo=FALSE}
rm(Normal, Pneumonia, filenames, out, list_subfolders)
```
To be able to load $5,500$ photos and make all required adjustments, the below function was created to make it possible to run through each URL, download zip, unfold it, load photos and adjust its size and color channels. 
```{r process folder function, echo=TRUE}
process_folder <- function(url, zip, folder) {

  # 1 Step - download ZIP
  drive_download(url)

  # 2 Step - extract files from archive (to working directory)
  out <- unzip(zip)

  # 3 Step - go to the folder and load all photos to R
  list_subfolders <- list.dirs(path = folder, full.names = TRUE, recursive = FALSE)
  filenames <- lapply(list_subfolders, function(x) list.files(path = x, pattern = "*.jpeg", full.names = TRUE))
  filenames <- do.call(c, filenames)

  # Function to load all photos to R, turn it into gray scale and resize
  temp <- lapply(filenames, function(x) {
    im <- load.image(x)
    im <- grayscale(im)
    im <- resize(im, 100, 100)
    temp <- c(as.data.frame(im)$value * 255.0, Y = basename(dirname(x)))
  })

  temp <- as.data.frame(do.call("rbind", temp))
  unlink(folder, recursive = TRUE) # delete temp photos
  file.remove(zip) # delete zip
  temp
}
```
\
Now let's use this function to load photos to R and create dataframes for each zip folder. 
Approximate time to run the code $\approx$ 25 minutes.
```{r photos upload to R, echo=TRUE, warning = FALSE}
val <- process_folder(val_url, val_zip, val_folder)
test <- process_folder(test_url, test_zip, test_folder)
train <- process_folder(train_url, train_zip, train_folder)
```

```{r remove unnecessary folders, echo=FALSE}
rm(test_folder, test_url, test_zip, train_folder, train_url, train_zip, val_folder, val_url, val_zip)
```
\
Below there are example of resized and adjusted photos for both categories.


```{r example resized photos, echo=FALSE}
rafalib::mypar()
normal <- 1
pneumonia <- 9

m1 <- matrix(val[normal, 1:10000], 100, 100)
mode(m1) <- "numeric"
m2 <- matrix(val[pneumonia, 1:10000], 100, 100)
mode(m2) <- "numeric"

layout(t(1:2))
image(m1, main = "Normal X-Ray example")
image(m2, main = "Pneumonia X-Ray example")
```

```{r remove unnecessary folders 2, echo=FALSE}
rm(m1, m2, normal, pneumonia)
```

##  Data Exploration

After we loaded and adjusted all photos, we can explore 3 datasets in details.
```{r dim, echo=TRUE}
dim(train)
dim(test)
dim(val)
```
\
Each dataset has $10,001$ variables with 100x100 pixels and label $Y$ (Normal or Pneumonia).\
However, we can notice that the data split is not aligned with the best practice rule 80-10-10 (80% train, 10% test, and 10% validation).

```{r split of datasets table, echo=FALSE}
data.frame(train = nrow(train), test = nrow(test), val = nrow(val)) %>%
  gather(Dataset, Obs) %>%
  mutate(percentage = round(Obs / sum(Obs), 3))
```
In addition, the graph below shows that the share of Pneumonia is higher in the Train set. This fact may cause models to over train and bias to detect pneumonia cases. We need to adjust split not only to 80-10-10 rules, but also that each set represents the population accurately.
```{r split of datasets, echo=FALSE}
train_stats <- train %>%
  group_by(Y) %>%
  summarize(n = n()) %>%
  mutate(type = "train", percent = n / sum(n))
test_stats <- test %>%
  group_by(Y) %>%
  summarize(n = n()) %>%
  mutate(type = "test", percent = n / sum(n))
val_stats <- val %>%
  group_by(Y) %>%
  summarize(n = n()) %>%
  mutate(type = "val", percent = n / sum(n))

all_db_stats <- rbind(train_stats, test_stats, val_stats)

all_db_stats %>% ggplot(aes(fill = Y, y = percent, x = type)) +
  geom_bar(position = "fill", stat = "identity") +
  geom_text(aes(label = paste0(sprintf("%1.1f", percent * 100), "%")), position = position_stack(vjust = 0.5), colour = "white", size = 5)
```

```{r delete temp 1,echo=FALSE}
rm(train_stats, test_stats, val_stats, all_db_stats)
```

##  Dataset re-split

To perform data re-split we will combine all 3 datasets and then split it into $train$ and $test$ (80/20) and then $test$ dataset into $test$ and $validation$ (10/10).
```{r re-split, echo=TRUE}
full_db <- rbind(train, test, val) # combine all photos

set.seed(2007)
test_index <- createDataPartition(full_db$Y, times = 1, p = 0.2, list = FALSE)

# Split db into train and test (80/20)

train_set_new <- full_db[-test_index, ]
test_set_temp <- full_db[test_index, ]

# Split test set into test and validation datasets (10/10)

val_index <- createDataPartition(test_set_temp$Y, times = 1, p = 0.5, list = FALSE)
test_set_new <- test_set_temp[-val_index, ]
val_set_new <- test_set_temp[val_index, ]

train_set_new[, 1:10000] <- sapply(train_set_new[, 1:10000], as.numeric)
test_set_new[, 1:10000] <- sapply(test_set_new[, 1:10000], as.numeric)
val_set_new[, 1:10000] <- sapply(val_set_new[, 1:10000], as.numeric)
```

Let's check if data in each set has the same share of Normal and Pneumonia photos now.
\
```{r resplit graph, echo=FALSE}
train_stats_new <- train_set_new %>%
  group_by(Y) %>%
  summarize(n = n()) %>%
  mutate(type = "train", percent = n / sum(n))
test_stats_new <- test_set_new %>%
  group_by(Y) %>%
  summarize(n = n()) %>%
  mutate(type = "test", percent = n / sum(n))
val_stats_new <- val_set_new %>%
  group_by(Y) %>%
  summarize(n = n()) %>%
  mutate(type = "val", percent = n / sum(n))

full_stats_new <- rbind(train_stats_new, test_stats_new, val_stats_new)

full_stats_new %>% ggplot(aes(fill = Y, y = percent, x = type)) +
  geom_bar(position = "fill", stat = "identity") +
  geom_text(aes(label = paste0(sprintf("%1.1f", percent * 100), "%")), position = position_stack(vjust = 0.5), colour = "white", size = 5)
```
\
We will also split $X$ (pixels) and $Y$ (labels) to be able to test models easier (from code perspective).
\
```{r delete temp 2, echo=FALSE }
rm(train_stats_new, test_stats_new, val_stats_new, full_stats_new)
rm(test_set_temp, test_index, val_index)
```
```{r x and y split, Echo=TRUE}
train_x <- train_set_new[, 1:10000]
train_y <- train_set_new[, 10001]

test_x <- test_set_new[, 1:10000]
test_y <- test_set_new[, 10001]

val_x <- val_set_new[, 1:10000]
val_y <- val_set_new[, 10001]
```


# Preprocessing data

## Remove not useful predictors

The area of our research is the chest and lungs. In the previous sections, we could see that chest X-Ray usually contains some other body parts such as the head and arms, but also edges of the photo are often empty. To make our models more efficient we need to get rid of not useful predictors.
```{r load lib, echo=FALSE}
library(matrixStats)
```

```{r zerovar, fig.width=5 ,echo=TRUE}
nzv <- nearZeroVar(train_x) # function recommends features to be removed due to near zero variance
image(matrix(1:10000 %in% nzv, 100, 100))
```
We can see that pixels on the edges of the X-Ray doesn't bring useful information. We will help the model to cut borders more as we see that around 10 pixels from left and right will not even touch the chest area.

```{r function to cut pixels, Echo=TRUE}
seq_new <- seq(0, 9900, 100)
pixels_to_cut <- c(0:9, 90:99)
new_nzv <- c()
for (i in seq_new) {
  for (j in pixels_to_cut) {
    new_nzv <- c(new_nzv, i + j)
  }
}
new_nzv <- c(new_nzv, 10000)

# Revised pixels crop
layout(t(1:2))
image(matrix(1:10000 %in% nzv, 100, 100), main = "First Crop")
image(matrix(1:10000 %in% new_nzv, 100, 100), main = "Revised Crop")
```

During closer photos exploration we might find that first bottom $2,000$ pixels usually contain the pelvic bone part of image, while top $2,000$ pixels cover the neck and head. These parts are not related to the topic of the research, so we can cut them as well.
```{r function to cut pixels revised, Echo=TRUE}
new_nzv <- c(new_nzv, c(1:2000), c(8000:10000))
new_nzv <- unique(new_nzv)

# Revised pixels crop - Final
layout(t(1:2))
image(matrix(1:10000 %in% nzv, 100, 100), main = "First Crop")
image(matrix(1:10000 %in% new_nzv, 100, 100), main = "Revised Crop Final")
```

The dimension of pixels left for analysis would be 80x60 (see example of new photos dimension below).

```{r cut photos examples, echo=FALSE}
normal <- 585
pneumonia <- 4290

m1 <- matrix(train_x[normal, !(1:10000 %in% new_nzv)], 80, 60)
mode(m1) <- "numeric"
m2 <- matrix(train_x[pneumonia, !(1:10000 %in% new_nzv)], 80, 60)
mode(m2) <- "numeric"

layout(t(1:2))
image(m1, main = "Normal")
image(m2, main = "Pneumonia")

rm(m1, m2)
```
```{r index 1, echo=TRUE}
col_index_1_cut <- setdiff(1:ncol(train_x), new_nzv) # indexes left after 1st cut
```
We will also save these features indexes for further analysis.


## Normalization

Before building any model, we need to normalize our factors by deducting $AVG$ and dividing by $SD$. It's required so as the algorithm would not be dominated by the variables using a larger scale, adversely affecting model performance.

```{r std div pic, fig.width=5, echo=TRUE}
colMeans <- colMeans(as.matrix(train_x))
colSds <- colSds(as.matrix(train_x))

layout(t(1:1))
image(matrix(colSds[col_index_1_cut], 80, 60), main = "Std deviation of pixels")
```
\
From the matrix of Standard deviations we can see that pixels in the area of heart and spine don't change much, however, there is more deviation around ribs area, and the highest is at the borders where photos usually differ from each other.

```{r hist of std div, echo=TRUE}
layout(t(1:2))

hist(colSds[col_index_1_cut], main = "Histogram of Std Deviation", xlab = "Std dev")

image(matrix(colSds[col_index_1_cut] < 30, 80, 60), main = "Pixels with Std dev < 30")
```
\
We can see that the $1st$ hill in the histogram of Std deviation is lowest deviation around heart and spine. It's another $1,202$ variables to exclude from the model ($sdt. dev<30$).
```{r}
index_to_exclude <- which(colSds[col_index_1_cut] < 30)
col_index <- col_index_1_cut[-index_to_exclude] # indexes left after final cut
```

Now we have only $3,598$ variables out of $10,000$ to analyze. Let's finish data preparation with normalization of our datasets.

```{r Normalization, echo=TRUE}
train_x_scaled <- sweep(sweep(train_x, 2, colMeans), 2, colSds, FUN = "/")
test_x_scaled <- sweep(sweep(test_x, 2, colMeans), 2, colSds, FUN = "/")
val_x_scaled <- sweep(sweep(val_x, 2, colMeans), 2, colSds, FUN = "/")
```

And test how our normalized images would look like.
```{r normalized photos, echo=FALSE}
normal <- 585
pneumonia <- 4290

m1 <- matrix(train_x_scaled[normal, !(1:10000 %in% new_nzv)], 80, 60)
mode(m1) <- "numeric"
m2 <- matrix(train_x_scaled[pneumonia, !(1:10000 %in% new_nzv)], 80, 60)
mode(m2) <- "numeric"

layout(t(1:2))
image(m1, main = "Normal")
image(m2, main = "Pneumonia")

rm(m1, m2)
```
We can see how spine and heart area is no longer dominant and photos became lighter in general.


# Models testing

## K-means

First, let's test different # of clusters for K-means and how the magnitude of $WSS$ (distance between data points the centroids of clusters) would change with # clusters.\

*Approx Time to run the for-loop $\approx$ 40 mins.*

```{r clusters test, echo=TRUE}
set.seed(3, sample.kind = "Rounding")

wss <- NULL
for (i in c(2, 4, 6, 8, 10, 12, 14)) {
  k <- kmeans(train_x_scaled[, col_index], centers = i, nstart = 25)
  wss <- c(wss, k$tot.withinss)
}
plot(c(2, 4, 6, 8, 10, 12, 14), wss, type = "o")
```

Based on the Elbow rule, $k=6$ would be an optimal # clusters: adding another cluster doesn’t improve the total $WSS$ much better.\
It's quite an interesting result due to the fact that we have just 2 categories: Normal and Pneumonia. Could the approach catch difference in X-Rays by kids' age? We can't say it due to lack of this information in labels.

*Approx Time to run the next code $\approx$ 10 mins.*
```{r k-means, echo=TRUE}
#K Means
k <- kmeans(train_x_scaled[, col_index], centers = 6, nstart = 25)

# Function to assign cluster
predict_kmeans <- function(x, k) {
  # extract cluster centers
  centers <- k$centers

  # calculate distance to cluster centers
  distances <- sapply(1:nrow(x), function(i) {
    apply(centers, 1, function(y) dist(rbind(x[i, ], y)))
  })

  # select cluster with min distance to center
  max.col(-t(distances))
}

y_hat_k_means <- predict_kmeans(test_x_scaled[, col_index], k)
y_hat_k_means <- ifelse(y_hat_k_means == 1, "NORMAL", "PNEUMONIA")

cm_k_means <- confusionMatrix(as.factor(y_hat_k_means), as.factor(test_y))
cm_k_means
F1_K_means <- F_meas(as.factor(y_hat_k_means), as.factor(test_y))
F1_K_means
```
\
We use $F_1$ score for accuracy assessment due to the fact that we have a not-balanced population of photos with and without Pneumonia. For us both Sensitivity (predict Normal X-Ray) and Specificity (predict Pneumonia X-Ray) are equally important. As we can see, K-Means couldn't recognize the difference between 2 classes and assigned the majority with the Pneumonia flag.
\


## Logistic Regression

To model the probability of a photo to be classified as Pneumonia we will test the Logistic model. Having quite a big number of variables, we will use $Glmnet$ package which can compute 10-fold cross-validation on massive matrices extremely fast.
```{r run logistic regression, echo=TRUE }
library(glmnet)
set.seed(1, sample.kind = "Rounding")

train_glmnet <- cv.glmnet(
  x = as.matrix(train_x_scaled[, col_index]), y = train_y,
  family = "binomial", type.measure = "class"
)
plot(train_glmnet)
```
\
The model will choose the regularization parameter $\lambda$ with the lowest misclassification error for the final computation via cross-validation.
```{r apply logistic regression, echo=TRUE}
train_glmnet$lambda.1se

y_hat_glmnet <- predict(train_glmnet, as.matrix(test_x_scaled[, col_index]), s = "lambda.1se", type = "class")
y_hat_glmnet <- as.vector(y_hat_glmnet)
cm_glmnet <- confusionMatrix(data = as.factor(y_hat_glmnet), reference = as.factor(test_y))
cm_glmnet

F1_glmnet <- F_meas(as.factor(y_hat_glmnet), as.factor(test_y))
F1_glmnet
```
\
From confusion matrix we can see how $F_1$ score has been improved in this model: $sensitivity = 75\%$ and $specificity = 97\%$.
\
If we look at the most important features as per logistic regression, we can find that pixels around ribs are significant in Pneumonia classification. The darker pixels are, the more likely the photo has the Pneumonia class. \
It's curious that the model has chosen dark pixels instead of light ones which form "clouds" in photos. It means that more important is to identify the edges of these "clouds", rather than "clouds" themselves.
```{r important features in glmnet, echo=FALSE}

glmnet_coef <- as.matrix(coef(train_glmnet, s = "lambda.1se"))

glmnet_coef <- glmnet_coef %>%
  data.frame() %>%
  mutate(feature = row.names(.)) %>%
  mutate(feature = str_replace(feature, "V", "")) %>%
  mutate(index = as.integer(feature))

glmnet_coef_whole_image <- left_join(data.frame(index = col_index_1_cut), glmnet_coef, by = "index")
glmnet_coef_whole_image[is.na(glmnet_coef_whole_image)] <- 0
layout(t(1:2))
image(matrix(colSds[col_index_1_cut], 80, 60), # most variable factors
  main = paste0("Most variable predictors")
)
image(matrix(glmnet_coef_whole_image$s1, 80, 60), # Highest coef importance
  main = paste0("Highest Coef importance in Log. Reg.")
) # the lighter the less probability it will have on Pneumonia
```
\


## Classification and Decision Tree

Classification Trees can fit high dimensional models with many predictors very well. Let's use cross-validation to make the model speed quicker and test 15 different complexity parameters $cp$ to avoid over-fitting. $cp$ specifies the bottom level of accuracy to be improved on each step of tree partition.

```{r CART, echo=TRUE}
set.seed(1, sample.kind = "Rounding")

control <- trainControl(method = "cv", number = 10, p = .9)

train_part <- train(
  x = train_x_scaled[, col_index], y = train_y,
  method = "rpart",
  tuneLength = 15, #tuning complexity parameter
  trControl = control
)

train_part
plot(train_part)
```

The plot above shows complexity parameters and corresponded accuracy of the model. Largest Accuracy has been achieved with $cp = 0.005134281$. We'll use this result in our final model.
\
*Approx Time to run the for-loop $\approx$ 7 mins.*
```{r CART_final, echo=TRUE}

y_hat_rpart <- predict(train_part, test_x_scaled[, col_index])

cm_rpart <- confusionMatrix(data = y_hat_rpart, reference = factor(test_y))
cm_rpart
F1_K_rpart <- F_meas(as.factor(y_hat_rpart), as.factor(test_y))
F1_K_rpart
```
\
A curious fact is that the model shows worse results in comparison to Logistic regression. It might happen because classes in our dataset are not well-separated, no clean relationship between variables and outcomes, trees are susceptible to overfitting the training data, so Logistic Regression's simple linear boundary generalizes better.

## kNN

Let's see what the most popular non-parametric model will tell us. \

The k-nearest Neighbors model ($kNN$) method is good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features. While non-parametric machine learning algorithms are often slower and require large amounts of data, they are rather flexible as they minimize the assumptions they make about the data.\

As previously, we will use cross-validation to protect us against over-fitting and tune parameter $k$ - number of neighbors.
```{r knn best k, echo=TRUE}
set.seed(1, sample.kind = "Rounding")
control <- trainControl(method = "cv", number = 10, p = .9) # Use 10-fold cross-validation to make speed quicker

train_knn <- train(train_x_scaled[, col_index], train_y,
  method = "knn",
  tuneGrid = data.frame(k = seq(1, 12, 2)), # To test different number of neighbors
  trControl = control
)
ggplot(train_knn)
train_knn$bestTune # find best number of neighbors (k)
train_knn$finalModel
```
\
Let's check the model on test set with chosen tuning parameter $k$. \
*The code below can take around 90 mins to run.*
```{r knn test, echo=TRUE}
y_hat_knn <- predict(train_knn, test_x_scaled[, col_index], type = "raw")
cm_knn <- confusionMatrix(data = y_hat_knn, reference = factor(test_y))
cm_knn

F1_knn <- F_meas(y_hat_knn, factor(test_y))
F1_knn
```
\
Again the model couldn't compete with Logistic regression but showed slightly better results than the classification tree. Could it mean that the relationship between important pixels and images is linear rather than nonlinear?
\
It's worth checking cases where we made mistakes with the following code:\
\
```{r mistakes, echo=FALSE}
fit_knn <- knn3(train_x_scaled[, col_index], factor(train_y), k = 5)
p_max <- predict(fit_knn, test_x_scaled[, col_index]) # Probability of prediction
p_max <- apply(p_max, 1, max)
ind <- which(y_hat_knn != factor(test_y)) # indexes where model failed
ind <- ind[order(p_max[ind], decreasing = TRUE)] # examples with highest errors
rafalib::mypar(2, 3)
for (i in ind[1:6]) {
  image(matrix(data.matrix(test_x_scaled[i, 1:10000]), 100, 100)[, 100:1],
    main = paste0(
      "Pr(", y_hat_knn[i], ")=", round(p_max[i], 2),
      " but is a ", test_y[i]
    ),
    xaxt = "n", yaxt = "n"
  )
}
```
Each of these photos has its own “defect”: one has a very light X-Ray, on another one - the kid’s position wasn’t straight enough when the scan was taken, etc. There will be room for improvement in our future research - to find a library and technologies which will allow us to center the photo properly and cut only the area required from the image.


## Random forest
\
And to finish our analysis we will go and test one of the most trustworthy models in data science - Random Forest.\
The strength of this model lies in creating different trees with different sub-features from the features. So first, we will tune $mtry$ - the number of variables randomly sampled as candidates at each split (tree).
```{r search mtry, echo=TRUE}
set.seed(9, sample.kind = "Rounding")

bestmtry <- tuneRF(train_x_scaled[, col_index], as.factor(train_y), stepFactor = 1.5, improve = 0.01, ntree = 500)
print(bestmtry)
```
\
In the results of the code we can see that the lowest $OOB$ (Out-of-bag) error (prediction error) appears with $88$ random variables for Random forest. We'll use it as input into our final model.
\
```{r rf_1, echo=TRUE}
bestmtry <- data.frame(bestmtry)
final_mtry <- bestmtry$mtry[which.min(bestmtry$OOBError)]

fit_rf <- randomForest(train_x_scaled[, col_index], as.factor(train_y),
  mtry = final_mtry, importance = TRUE
)
plot(fit_rf)
fit_rf
```
\
However, there is room for improvement here: let's try to make the estimate smoother by changing # of data points in nodes and choosing the one achieving the highest accuracy.
\
*Approx Time to run the for-loop $\approx$ 2 hrs.*
```{r change in nodes, echo=TRUE}

nodesize <- seq(2, 52, 10)

rf_accuracy <- sapply(nodesize, function(ns) {
  fit_rf_temp <- randomForest(train_x_scaled[, col_index], as.factor(train_y),
    mtry = final_mtry,
    nodesize = nodesize,
    importance = TRUE
  )

  y_hat_rf_temp <- predict(fit_rf_temp, test_x_scaled[, col_index])
  F_meas(y_hat_rf_temp, as.factor(test_y))
})

qplot(nodesize, rf_accuracy)

nodesize_best <- nodesize[which.max(rf_accuracy)]
nodesize_best
```
\
Based on these calculations, we can find the best node size which will maximize $F_1$ index. We'll use it in our final model for Random Forest.
\
*Approx Time to run the for-loop $\approx$ 20 mins.*
```{r rf final, echo=TRUE}
fit_rf <- randomForest(train_x_scaled[, col_index], as.factor(train_y),
  mtry = final_mtry,
  nodesize = nodesize_best,
  importance = TRUE
)

varImpPlot(fit_rf) # most important variables
```
\
Now, let's export the most important variables indexes and the Mean Decrease Gini index.\
Mean Decrease Gini is a measure of variable importance based on the Gini impurity index. Gini impurity index calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly. If all the elements are linked with a single class then it can be called pure. The higher MDGini is, the higher the importance of the variable in the model.
\
```{r export Gini, echo=TRUE}
# Export important variables indexes and Mean Decrease Gini index
feat_imp_df <- importance(fit_rf) %>%
  data.frame() %>%
  mutate(feature = row.names(.)) %>%
  mutate(feature = str_replace(feature, "V", "")) %>%
  mutate(index = as.integer(feature))

MDGini <- left_join(data.frame(index = col_index_1_cut), feat_imp_df, by = "index")
MDGini[is.na(MDGini)] <- 0

layout(t(1:2))
image(matrix(colSds[col_index_1_cut], 80, 60),
  main = paste0("Most variable predictors")
)
image(matrix(MDGini$MeanDecreaseGini, 80, 60),
  main = paste0("Highest Gini index in Random Forest")
)
```
\
We can see that results of Random Forest model are similar to kNN as both highlight the same areas to be important.
\
```{r final RF, echo=FALSE}
y_hat_rf <- predict(fit_rf, test_x_scaled[, col_index])
cm_rf <- confusionMatrix(y_hat_rf, as.factor(test_y))
cm_rf
F1_RF <- F_meas(y_hat_rf, as.factor(test_y))
F1_RF
```
\
However, we notice that sensitivity has been improved dramatically (by $+10 pp$ vs kNN, by $+2 pp$ vs Glmnet). But we also see that we predict fewer Pneumonia cases accurately (Neg pred value).\

What if we combine both best models ($F_1>=80 \%$) to get even better results?
\


## GLM + Random Forest
To merge 2 models, we'll need to combine its probabilities to have pneumonia and make average of the result.
```{r essemble, echo=TRUE}

p_glmnet <- predict(train_glmnet, as.matrix(test_x_scaled[, col_index]), s = "lambda.1se", type = "response")
p_rf <- predict(fit_rf, test_x_scaled[, col_index], type = "prob")

p <- (p_rf[, 2] + p_glmnet) / 2

y_ensemble_hat <- as.factor(ifelse(p >= 0.5, 2, 1))
test_y_factor <- as.factor(ifelse(test_y == "NORMAL", 1, 2))

cm_ensemble <- confusionMatrix(y_ensemble_hat, test_y_factor)
cm_ensemble
F1_ensemble <- F_meas(y_ensemble_hat, test_y_factor)
F1_ensemble
```


## Final model choice and application

Let's compare all models we have and make a decision about the best one.
\
```{r all models, echo=FALSE}
results <- data.frame(GlmNet = round(F1_glmnet, 3), Rpart = round(F1_K_rpart, 3), KNN = round(F1_knn, 3), RF = round(F1_RF, 3), Glm_RF = round(F1_ensemble, 3))
results <- results %>%
  gather(Model, F1) %>%
  arrange(desc(F1))
results
```
\
We can see that the combined model from Logistic regression (GlmNet) and Random Forest (RF) increases accuracy up to $85\%$ and become the model of our choice (for now). Let's apply it to our $val$ dataset and see the final result of our research.
```{r final, echo=FALSE}
p_glmnet_final <- predict(train_glmnet, as.matrix(val_x_scaled[, col_index]), s = "lambda.1se", type = "response")
p_rf_final <- predict(fit_rf, val_x_scaled[, col_index], type = "prob")
p_final <- (p_rf_final[, 2] + p_glmnet_final) / 2

y_hat_val <- as.factor(ifelse(p_final>= 0.5, 2, 1))
val_y_factor<- as.factor(ifelse(val_y == "NORMAL", 1, 2))

cm_final <- confusionMatrix(y_hat_val, val_y_factor)
cm_final
F1_final <- F_meas(y_hat_val, val_y_factor)
F1_final
```


# Summary and Future perspective

As a result of our investigation, we have managed to create a model which will help to detect Pneumonia diagnosis much easier and with quite a high accuracy. However, there are possibilities for improvement.

Image recognition topic is quite challenging especially with examples from real life when data requires a lot of cleaning.
In our case, it would make the model more accurate if we could cut and extract only "chest-ribs" shape from the X-Ray, centralize them and compare them to each other. As in our basic modeling, we cut the initial photos and didn't move the image around the center.

Another point to keep in mind is that our data contained photos of children of age between 1 and 5 years old. It would be great to have labels specifying age as well, not only diagnosis, as we think that the ratio of heart size to the chest area can differ with age and might affect our results.

It's also clear that in future research we will need to try more advanced models such as CNN (Convolutional Neural Network) and other layers-based Neural networks to increase accuracy up to 90%. These models tend to work very efficiently with high-dimensional datasets like photos and might help improve the accuracy by at least increasing the resolution of the images as in our case it was quite challenging to catch these "foggy" anomalies appearing on X-Ray.

Thanks a lot for your attention!

Feel free to reach out to me at $julia.d.shal@gmail.com$ if you have any questions or suggestion regarding this research.
\
\
\
\
\

Acknowledgement:\
https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia

https://www.who.int/health-topics/pneumonia#tab=tab_1

Artificial Intelligence: A Modern Approach - Stuart Russell